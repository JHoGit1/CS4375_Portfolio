# S23_CS4395_Portfolio
A portfolio for the CS4395 HLT (NLP) class. 

## Component 0
This assignment is an introduction to what NLP is and my personal short view NLP.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%200/C0_Jonathan_Ho_Overview_of_NLP.pdf) to view the short document.

## Component 1
This assignment is utilizing Python to create a simple script that can take in and process data from a .csv file.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%201/jqh200000_A1.py) to see the python script.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%201/C1_Jonathan_Ho_Python_Overview.pdf) for a small overview about the script.

## Component 2
For this assignment, a Python script was created to be able to preprocess and tokenize a text file, find the 50 most common used nouns, and pick one word from the common nouns to use for a guessing game.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%202/jqh200000_A2.py) to see the python script for the guessing game.

## Component 3
For this assignment, the WordNet package from NLTK is explored using Google Colab. 

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%203/C3_Jonathan_Ho_WordNet.pdf) to see the notebook using WordNet.

## Component 4
Exploration of Ngrams were conducted by utilizing Python code. The code is split into two programs. The first program took in texts to create unigram and bigram dictionaries and pickled them. Then, the second program would unpickle the dictionaries to interpret a test file with lines of languages varying between English, French, and Italian. The language model would determine the probabilities of each language for each line and output to a text file what language it thinks that test line is. An overview narrative was then written.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%204/jqh200000_A4_P1.py) for the code of the first program.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%204/jqh200000_A4_P2.py) for the code of the second program.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%204/jqh200000_A4_Narrative.pdf) for the narrative on ngrams.

## Component 5
Different sentence parsing techniques were viewed in this assignment. Through AllenNLP, PSG, dependancy, and SRL parsing were carried out on a complex sentence.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%205/jqh200000_A5_Writeup.pdf) to view the write up on the different parsers.

## Component 6
A web crawler was created to look for links with a term, find the text within the links, clean up the links, and find their term frequencies. Ten significant terms were chosen across the links, and a knowledge base was formulated using a simple Python dictionary.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%206/jqh200000_A6.py) to view the code for the web crawler.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%206/jqh200000_A6_Knowledge_Base_Writeup.pdf) to view the short write up of the knowledge base.

## Component 7
Naive Bayes, Logistic Regression, and Neural Networks were used to create models to predict text classification. 

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%207/jqh200000_A7.pdf) to view the notebok for the text classification.

## Component 8
An peer-reviewed paper was chosen from the ACL Anthology website to read and summarize. The link for the specific paper that was read is [here](https://aclanthology.org/2022.acl-long.27/). 

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%208/jqh200000_A8.pdf) to see the summary of the ACL paper chosen.