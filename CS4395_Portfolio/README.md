# S23_CS4395_Portfolio
A portfolio for the CS4395 HLT (NLP) class. 

## Component 0
This is an introduction to what NLP is and my personal short view NLP.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%200/C0_Jonathan_Ho_Overview_of_NLP.pdf) to view the short document.

## Component 1
The following portfolio component utilizes Python to create a simple script that can take in and process data from a .csv file.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%201/jqh200000_A1.py) to see the python script.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%201/C1_Jonathan_Ho_Python_Overview.pdf) for a small overview about the script.

## Component 2
In this component, a Python script was created to be able to preprocess and tokenize a text file, find the 50 most common used nouns, and pick one word from the common nouns to use for a guessing game.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%202/jqh200000_A2.py) to see the python script for the guessing game.

## Component 3
Google Colab was used to explore the WordNet package from NLTK. Elements that were explored were synsets, word similarity checks, and word sentiment values.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%203/C3_Jonathan_Ho_WordNet.pdf) to see the notebook using WordNet.

## Component 4
Exploration of Ngrams were conducted by utilizing Python code. The code is split into two programs. The first program took in texts to create unigram and bigram dictionaries and pickled them. Then, the second program would unpickle the dictionaries to interpret a test file with lines of languages varying between English, French, and Italian. The language model would determine the probabilities of each language for each line and output to a text file what language it thinks that test line is. An overview narrative was then written.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%204/jqh200000_A4_P1.py) for the code of the first program.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%204/jqh200000_A4_P2.py) for the code of the second program.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%204/jqh200000_A4_Narrative.pdf) for the narrative on ngrams.

## Component 5
Different sentence parsing techniques were viewed in this assignment. Through AllenNLP, PSG, dependancy, and SRL parsing were carried out on a complex sentence.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%205/jqh200000_A5_Writeup.pdf) to view the write up on the different parsers.

## Component 6
A web crawler was created to look for links with a term, find the text within the links, clean up the links, and find their term frequencies. Ten significant terms were chosen across the links, and a knowledge base was formulated using a simple Python dictionary.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%206/jqh200000_A6.py) to view the code for the web crawler.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%206/jqh200000_A6_Knowledge_Base_Writeup.pdf) to view the short write up of the knowledge base.

## Component 7
Naive Bayes, Logistic Regression, and Neural Networks were used to create models to predict text classification. 

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%207/jqh200000_A7.pdf) to view the notebok for the text classification.

## Component 8
An peer-reviewed paper was chosen from the ACL Anthology website to read and summarize. The link for the specific paper that was read is [here](https://aclanthology.org/2022.acl-long.27/). 

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%208/jqh200000_A8.pdf) to see the summary of the ACL paper chosen.

## Component 9

A Simple Recipe Chatbot was created using Google's DialogFlow ES. The project was done in collaboration with David Park.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/tree/main/CS4395_Portfolio/Component%209) to view the source code and static knowledge base of the chatbot.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%209/Chatbot_Report.pdf) to view the writeup of the chatbot.

## Component 10
Different deep learning models were created to try and predict whether or not text was spam. This includes a sequential model, RNN, CNN, LSTM, and GRU as well as a GloVe embedding.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%2010/jqh200000_A10.pdf) to see the overview.